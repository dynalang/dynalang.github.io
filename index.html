<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta content="Dynalang leverages diverse types of language to solve tasks by using language to predict the future in a multimodal world model."
    name="description" />
  <meta content="Dynalang: Learning to Model the World with Language" property="og:title" />
  <meta content="Dynalang leverages diverse types of language to solve tasks by using language to predict the future in a multimodal world model."
    property="og:description" />
  <meta content="https://dynalang.github.io/data/open_graph.png" property="og:image" />
  <meta content="Learning to Model the World with Language" property="twitter:title" />
  <meta content="Dynalang leverages diverse types of language to solve tasks by using language to predict the future in a multimodal world model." property="twitter:description" />
  <meta name="twitter:site" content="@realJessyLin" />
  <meta name="twitter:creator" content="@realJessyLin" />
  <meta content="https://dynalang.github.io/data/open_graph.png" property="twitter:image" />
  <meta property="og:type" content="website" />
  <meta content="summary_large_image" name="twitter:card" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />


  <title>Learning to Model the World with Language</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Asap:wght@700&family=Source+Sans+3:wght@400;700&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Asap:wght@700&family=Source+Sans+3:wght@400;700&display=swap">
  <link href="style.css" rel="stylesheet" type="text/css" />
</head>

<body>


  <div class="section">
    <div class="container">
      <div class="title-row">
        <h1 class="title">Learning to Model the World with Language<h1>
      </div>
      <div class="row">
        <div class="author-col">
          <a href="https://www.jessylin.com/" target="_blank" class="author-text">
            Jessy Lin
          </a>
        </div>
        <div class="author-col">
          <a href="https://www.yuqingd.github.io/" target="_blank" class="author-text">
            Yuqing Du
          </a>
        </div>
        <div class="invisible"><br></div>
        <div class="author-col">
          <a href="https://aliengirlliv.github.io/oliviawatkins/" target="_blank" class="author-text">
            Olivia Watkins
          </a>
        </div>
        <div class="author-col">
          <a href="https://danijar.com/" target="_blank" class="author-text">
            Danijar Hafner
            <span class="superscript"></span>
          </a>
        </div>
      </div>
      <div class="row">
        <div class="author-col">
          <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank" class="author-text">
            Pieter Abbeel
          </a>
        </div>
        <div class="author-col">
          <a href="https://people.eecs.berkeley.edu/~klein/" target="_blank" class="author-text">
            Dan Klein
          </a>
        </div>
        <div class="author-col">
          <a href="https://people.eecs.berkeley.edu/~anca/" target="_blank" class="author-text">
            Anca Dragan
          </a>
        </div>
      </div>

      </div>
      <p id="uc-berkeley">UC Berkeley</h1>

      <div class="row">
        <a class="link-button" href="data/dynalang.pdf" target="_blank" class="link-block">Paper</a>
        <a class="link-button" href="https://github.com/jlin816/dynalang" class="link-block">Code</a>
      </div>
      <p class="tldr">
        <b>TL;DR</b>:
        Dynalang leverages diverse types of language to solve tasks by using language to predict the future in a multimodal world model.
      </p>
      <video id="main-video" muted autoplay controls playsinline loop>
        <source id="mp4" src="data/videos/teaser.mp4" type="video/mp4">
      </video>

      <div id="content">
        <h2 class="section-header">Overview</h2>
        <div class="paragraph">
          <p>
          To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that <b>language helps agents predict the future</b>: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to predict future language, video, and rewards. In addition to learning from online interaction in an environment, Dynalang can be pretrained on datasets of text, video, or both without actions or rewards. From using language hints in grid worlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types of language to improve task performance, including environment descriptions, game rules, and instructions.
          </p>
          <p><b>Contributions:</b></p>
          <ul>
            <li>We propose Dynalang, an agent that grounds language to visual experience via future prediction.
            <li>We demonstrate that Dynalang learns to understand diverse kinds of language to solve a broad range of tasks, often outperforming state-of-the-art RL algorithms and task-specific architectures.
            <li>We show that the Dynalang formulation enables additional capabilities: language generation can be unified in the same model, as well as text-only pretraining without actions or task rewards.
          </ul>
        </div>

        <h2 class="section-header">How It Works</h2>
        <div class="img-container">
          <div class="paragraph">
          During online interaction, Dynalang is trained continuously from experience data the agent collects while acting in the environment. We treat language as an observation modality, inputting it into the world model along with image observations. Dynalang receives one token at each timestep, which matches how agents would receive streams of multimodal observations in the real world and enables the agent to take actions while reading text.
          </div>
          <img class="wide-img" src="static/images/model.png" alt="Dynalang Model Architecture">
          <div class="paragraph">
            (left) The world model compresses the text and image at each timestep to a latent representation. From the representation, the model is trained to reconstruct the original observations, predict rewards, and predict the representation at the next timestep.
            Intuitively, the world model learns <b>what it should expect to see in the world</b> given what it reads in text. (right) The policy operates on top of the compressed world model representations. It is trained on imagined rollouts from the world model and learns to take actions that maximize predicted returns.
          </div>
        </div>

        <h2 class="section-header">Language Hints in HomeGrid</h2>
        <div class="paragraph">
          We introduce HomeGrid to evaluate agents in an environment where they receive <i>language hints</i> in addition to task instructions. Hints in HomeGrid simulate knowledge that agents might learn from humans or read in text, providing information that is helpful but not required to solve tasks:
        <ul>
          <li><b>Future Observations</b> describe what agents might observe in the future, such as "The plates are in the kitchen."</li>
          <li><b>Corrections</b> provide interactive feedback based on what the agent is doing, such as "Turn around."</li>
          <li><b>Dynamics</b> describe the dynamics of the environment, such as "Pedal to open the compost bin."</li>
        </ul>
        </div>
        <div class="paragraph">The HomeGrid environment will be released with the code to encourage further work in this direction.</div>

        <div class="videos">
          <div>
            <video muted autoplay controls playsinline loop>
              <source id="mp4" src="data/videos/homegrid_future.mp4" type="video/mp4">
            </video>
            <p class="caption">Future Observations</p>
          </div>
          <div>
            <video muted autoplay controls playsinline loop>
              <source id="mp4" src="data/videos/homegrid_corrections.mp4" type="video/mp4">
            </video>
            <p class="caption">Corrections</p>
          </div>
          <div>
            <video muted autoplay controls playsinline loop>
             <source id="mp4" src="data/videos/homegrid_dyn.mp4" type="video/mp4">
            </video>
            <p class="caption">Dynamics</p>
          </div>
        </div>

        <div class="paragraph">
          Even though agents do not receive explicit supervision for what observations a piece of text corresponds to, Dynalang learns to ground language of all types to the environment via the future prediction objective. Dynalang outperforms language-conditioned IMPALA and R2D2 that struggle to use different kinds of language and often do worse with language beyond instructions.
        </div>

        <img class="wide-img" src="static/images/homegrid_bars.png" alt="HomeGrid Results">

        <h2 class="section-header">Game Manuals in Messenger</h2>
        <div class="paragraph">
        We evaluate on the Messenger game environment to test how agents learn from longer and more complex text that requires multi-hop reasoning over text and visual observations. Agents must reason over text manuals describing the dynamics of each episode and combine them with observations of the entities in the environment to determine which entities to get messages from and which to avoid. Dynalang outperforms IMPALA and R2D2, as well as the task-specific EMMA baseline that uses a specialized architecture to reason over text and observations, particularly on the most difficult Stage 3.
        </div>
        <img class="med-img" src="static/images/messenger_curves.png" alt="Messenger Results">
        <div class="videos">
          <div>
          <video muted autoplay controls playsinline loop>
            <source id="mp4" src="data/videos/s30.mp4" type="video/mp4">
          </video>
          </div>
          <div>
          <video muted autoplay controls playsinline loop>
            <source id="mp4" src="data/videos/s31.mp4" type="video/mp4">
          </video>
          </div>
          <div>
          <video muted autoplay controls playsinline loop>
            <source id="mp4" src="data/videos/s32.mp4" type="video/mp4">
          </video>

          </div>
        </div>

        <h2 class="section-header">Instruction Following in Habitat</h2>
        <div class="img-by-text">
          <img class="sm-img" src="static/images/vln.png" alt="Vision-Language Navigation Results">
          <div class="paragraph">
          We also show that Dynalang is able to handle photorealistic visual observations and perform instruction following in Habitat. Agents must follow natural language instructions to navigate to a goal location in a photorealistic scan of a home. In Dynalang, instruction following can be unified in the same prediction framework by viewing it as future reward prediction.
          </div>
        </div>
        <div class="videos">
          <div>
          <video muted autoplay controls playsinline loop>
            <source id="mp4" src="data/videos/vln1.mp4" type="video/mp4">
          </video>
          </div>
          <div>
          <video muted autoplay controls playsinline loop>
            <source id="mp4" src="data/videos/vln0.mp4" type="video/mp4">
          </video>
          </div>
          <div>
          <video muted autoplay controls playsinline loop>
            <source id="mp4" src="data/videos/vln3.mp4" type="video/mp4">
          </video>
          </div>
        </div>

        <h2 class="section-header">Grounded Language Generation in LangRoom</h2>
        <div class="paragraph">
          Just like language can affect agents' predictions of what they will see, what an agent observes can also affect what language it expects to hear (e.g., true statements about what it has seen). By outputting language in the action space in LangRoom, we show that Dynalang can <i>generate</i> language grounded in environment to perform embodied question answering.
        </div>
        <div class="img-by-text">
          <img class="sm-img" src="static/images/langroom.png" alt="LangRoom Results">
          <div id="langroom-vid">
          <video muted autoplay controls playsinline loop style="width: 100%">
            <source id="mp4" src="data/videos/langroom.mp4" type="video/mp4">
          </video>
        </div>
        </div>


        <h2 class="section-header">Text Pretraining</h2>
        <div class="paragraph">
          Because world modeling with language is decoupled from learning to act with a world model, Dynalang can be pretrained with offline data without action or reward labels. This capability provides a way for Dynalang to benefit from large-scale offline datasets, all within a single model architecture. We pretrain Dynalang with text-only data, learning token embeddings from scratch. Pretraining the model on general text data (TinyStories, 2M short stories) improves downstream RL task performance on Messenger beyond using pretrained T5 embeddings.
        </div>
        <img class="med-img" src="static/images/text_pretrain_all.png" alt="Text Pretraining Results">
        <div class="paragraph">Although our work focuses on language understanding for acting in world, we can generate text from the world model like a text-only language model. We sample rollouts from the pretrained TinyStories model in latent space and decode the token observation from the representation at each timestep. Model generations are surprisingly coherent, although still below the quality of modern language models. We see unifying language generation and acting in a single agent architecture as an exciting avenue for future work.</div>
        <div id="text-samples">
          <div class="text-sample">
            <div class="label">Prompt</div>
            <div class="prompt bubble"> One day, a young boy named Tim found a dull, round rock. He picked it up and looked at it. He thought it was not very fun, but he took it with him to the park. At the park, Tim</div>
            <div class="label">True</div>
            <div class="true bubble"> saw a girl named Sue. She had</div>
            <div class="label">Model Samples</div>
            <ul class="samples">
              <li class="bubble">he met. favorite friend He put it his to</li>
              <li class="bubble">met a girl named Sue. Sue saw the ball</li>
              <li class="bubble">saw a stick top Sam. He kept playing with</li>
              <li class="bubble">played with his friends and but they friends!"</s> Li</li>
              <li class="bubble">met a girl named Lily.&lt;/s&gt; ly saw</li>
            </ul>
          </div>
          <div class="text-sample">
            <div class="label">Prompt</div>
            <div class="prompt bubble">Once upon a time, there was a little boy named Tom. Tom had a special belt that he loved to wear. One day, he could not find his belt and felt very sad. Tom's mom saw him and</div>
            <div class="label">True</div>
            <div class="true bubble"> asked, "Why are you sad, Tom?"</div>
            <div class="label">Model Samples</div>
            <ul class="samples">
              <li class="bubble">frustrated and asked him what was rude.</s> Once upon</li>
              <li class="bubble">asked, "Why are you sad, Tom?"&lt;/s&gt;</li>
              <li class="bubble">asked, "Howeny, I did, get</li>
              <li class="bubble">said, "Don't worry, Tom. We</li>
              <li class="bubble">said, "To tree, you look be  in</li>
            </ul>
          </div>
        </div>

        <h2 class="section-header">Citation</h2>
        <div class="paragraph-center">For more information, check out the paper and code:</div>
        <div class="row">
          <a class="link-button" href="data/dynalang.pdf" target="_blank" class="link-block">Paper</a>
          <a class="link-button" href="https://github.com/jlin816/dynalang" class="link-block">Code</a>
        </div>
        <div class="citation">
          <pre id="codecell0">@article{lin2023dynalang,
         author = {Lin, Jessy and Du, Yuqing and Watkins, Olivia and Hafner, Danijar and Abbeel, Pieter and Klein, Dan and Dragan, Anca},
         title = {Learning to Model the World with Language},
         note = {Preprint},
         year = {2023},
}</pre>
        </div>
      </div>

    </div>
  </div>
</body>

</html>
